{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cannot model the characters directly, instead we must convert the characters to integers. \n",
    "# We can do this easily by first creating a set of all of the distinct characters in the book, \n",
    "# then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Characters: ', 173595)\n",
      "('Total Vocab: ', 66)\n"
     ]
    }
   ],
   "source": [
    "# Summarize dataset\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we see that there are 66 distinct characters in vocab for network to learn.\n",
    "# we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. \n",
    "# We could just as easily split the data up by sentences and pad the shorter sequences and truncate the longer ones.\n",
    "# When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Patterns: ', 173495)\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length] # the characters in sequence\n",
    "\tseq_out = raw_text[i + seq_length] # The sequence position\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to transform it so that it is suitable for use with Keras. \n",
    "# First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "# Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "# Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. \n",
    "# This is so that we can configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each y value is converted into a sparse vector with a length of 47, \n",
    "# full of zeros except with a 1 in the column for the letter (integer) that the pattern represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 62, 63, 46, 48, 45, 40, 35, 33, 50, 2, 37, 51, 50, 35, 44, 32, 35, 48, 37, 64, 57, 59, 49, 2, 31, 42, 39, 33, 35, 64, 57, 59, 49, 2, 31, 34, 52, 35, 44, 50, 51, 48, 35, 49, 2, 39, 44, 2, 53, 45, 44, 34, 35, 48, 42, 31, 44, 34, 10, 2, 32, 55, 2, 42, 35, 53, 39, 49, 2, 33, 31, 48, 48, 45, 42, 42, 1, 0, 1, 0, 50, 38, 39, 49, 2, 35, 32, 45, 45, 41, 2, 39, 49, 2, 36, 45, 48, 2, 50]\n",
      "(173495, 100)\n"
     ]
    }
   ],
   "source": [
    "print(dataX[0])\n",
    "print(numpy.shape(dataX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173495, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "#print(X[0])\n",
    "print(numpy.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.8567Epoch 00000: loss improved from inf to 2.85645, saving model to weights-improvement-00-2.8564-bigger.hdf5\n",
      "173495/173495 [==============================] - 611s - loss: 2.8564   \n",
      "Epoch 2/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.4693Epoch 00001: loss improved from 2.85645 to 2.46933, saving model to weights-improvement-01-2.4693-bigger.hdf5\n",
      "173495/173495 [==============================] - 609s - loss: 2.4693   \n",
      "Epoch 3/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.3113Epoch 00002: loss improved from 2.46933 to 2.31130, saving model to weights-improvement-02-2.3113-bigger.hdf5\n",
      "173495/173495 [==============================] - 617s - loss: 2.3113   \n",
      "Epoch 4/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.1892Epoch 00003: loss improved from 2.31130 to 2.18917, saving model to weights-improvement-03-2.1892-bigger.hdf5\n",
      "173495/173495 [==============================] - 618s - loss: 2.1892   \n",
      "Epoch 5/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.0892Epoch 00004: loss improved from 2.18917 to 2.08918, saving model to weights-improvement-04-2.0892-bigger.hdf5\n",
      "173495/173495 [==============================] - 546s - loss: 2.0892   \n",
      "Epoch 6/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.0119Epoch 00005: loss improved from 2.08918 to 2.01181, saving model to weights-improvement-05-2.0118-bigger.hdf5\n",
      "173495/173495 [==============================] - 576s - loss: 2.0118   \n",
      "Epoch 7/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.9507Epoch 00006: loss improved from 2.01181 to 1.95069, saving model to weights-improvement-06-1.9507-bigger.hdf5\n",
      "173495/173495 [==============================] - 590s - loss: 1.9507   \n",
      "Epoch 8/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.9035Epoch 00007: loss improved from 1.95069 to 1.90354, saving model to weights-improvement-07-1.9035-bigger.hdf5\n",
      "173495/173495 [==============================] - 583s - loss: 1.9035   \n",
      "Epoch 9/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.8563Epoch 00008: loss improved from 1.90354 to 1.85628, saving model to weights-improvement-08-1.8563-bigger.hdf5\n",
      "173495/173495 [==============================] - 587s - loss: 1.8563   \n",
      "Epoch 10/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.8140Epoch 00009: loss improved from 1.85628 to 1.81396, saving model to weights-improvement-09-1.8140-bigger.hdf5\n",
      "173495/173495 [==============================] - 586s - loss: 1.8140   \n",
      "Epoch 11/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.7815Epoch 00010: loss improved from 1.81396 to 1.78146, saving model to weights-improvement-10-1.7815-bigger.hdf5\n",
      "173495/173495 [==============================] - 588s - loss: 1.7815   \n",
      "Epoch 12/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.7428Epoch 00011: loss improved from 1.78146 to 1.74288, saving model to weights-improvement-11-1.7429-bigger.hdf5\n",
      "173495/173495 [==============================] - 579s - loss: 1.7429   \n",
      "Epoch 13/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.7131Epoch 00012: loss improved from 1.74288 to 1.71306, saving model to weights-improvement-12-1.7131-bigger.hdf5\n",
      "173495/173495 [==============================] - 566s - loss: 1.7131   \n",
      "Epoch 14/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.6908Epoch 00013: loss improved from 1.71306 to 1.69082, saving model to weights-improvement-13-1.6908-bigger.hdf5\n",
      "173495/173495 [==============================] - 568s - loss: 1.6908   \n",
      "Epoch 15/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.6604Epoch 00014: loss improved from 1.69082 to 1.66036, saving model to weights-improvement-14-1.6604-bigger.hdf5\n",
      "173495/173495 [==============================] - 568s - loss: 1.6604   \n",
      "Epoch 16/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.6366Epoch 00015: loss improved from 1.66036 to 1.63653, saving model to weights-improvement-15-1.6365-bigger.hdf5\n",
      "173495/173495 [==============================] - 565s - loss: 1.6365   \n",
      "Epoch 17/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.6155Epoch 00016: loss improved from 1.63653 to 1.61550, saving model to weights-improvement-16-1.6155-bigger.hdf5\n",
      "173495/173495 [==============================] - 568s - loss: 1.6155   \n",
      "Epoch 18/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.5952Epoch 00017: loss improved from 1.61550 to 1.59525, saving model to weights-improvement-17-1.5953-bigger.hdf5\n",
      "173495/173495 [==============================] - 565s - loss: 1.5953   \n",
      "Epoch 19/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.5747Epoch 00018: loss improved from 1.59525 to 1.57470, saving model to weights-improvement-18-1.5747-bigger.hdf5\n",
      "173495/173495 [==============================] - 567s - loss: 1.5747   \n",
      "Epoch 20/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.5594Epoch 00019: loss improved from 1.57470 to 1.55939, saving model to weights-improvement-19-1.5594-bigger.hdf5\n",
      "173495/173495 [==============================] - 561s - loss: 1.5594   \n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating text with LSTM model\n",
    "### Firstly, we load the data and define the network in exactly the same way, except the network weights are loaded from a \n",
    "### checkpoint file and the network does not need to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-19-1.5594-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to \n",
    "# convert the integers back to characters so that we can understand the predictions.\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, \n",
    "# generate the next character then update the seed sequence to add the generated character on the end and trim off the \n",
    "# first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "('\"', 'te that she was ready to ask help of any one; so, when the rabbit\\r\\ncame near her, she began, in a lo', '\"')\n",
      "ngnt ald the mint of the sooe, \n",
      "\n",
      "���i don���t know the ming,��� said the mock turtle.�i shall seres out\n",
      "the mart her ie in the soialers!��� \n",
      "\n",
      "���i don���t know the mintte of the soaate of the sorp!��� said the mock turtle\n",
      "aniner. ���i shall have to be a gat of the sorp!��� \n",
      "\n",
      "���i don���t know the mintte of the soaate of the soaae,��� said the mock turtle\n",
      "anine. \n",
      "\n",
      "���i don���t know the mintte of the soaate of the soaae,��� said the mock turtle\n",
      "anine. \n",
      "\n",
      "���i don���t know the mintte of the soaate of the soaae,��� said the mock turtle\n",
      "anine. \n",
      "\n",
      "���i don���t know the mintte of the soaate of the soaae,��� said the mock turtle\n",
      "anine. \n",
      "\n",
      "���i don���t know the mintte of the soaate of the soaae,��� said the mock turtle\n",
      "anine. \n",
      "\n",
      "���i don���t know the mintte of the soaate of the soaae,��� said the mock turtle\n",
      "anine. \n",
      "\n",
      "���i don���t know the mintte of the soaate of the soaae,��� said the mock turtle\n",
      "anine. \n",
      "\n",
      "���i don���t know the mintte of the soaate of the soaae,��� said the moc\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# We can pick a random input pattern as our seed sequence, then print generated characters as we generate them.\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
