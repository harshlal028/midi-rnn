{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cannot model the characters directly, instead we must convert the characters to integers. \n",
    "# We can do this easily by first creating a set of all of the distinct characters in the book, \n",
    "# then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Characters: ', 173595)\n",
      "('Total Vocab: ', 66)\n"
     ]
    }
   ],
   "source": [
    "# Summarize dataset\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we see that there are 66 distinct characters in vocab for network to learn.\n",
    "# we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. \n",
    "# We could just as easily split the data up by sentences and pad the shorter sequences and truncate the longer ones.\n",
    "# When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Patterns: ', 173495)\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length] # the characters in sequence\n",
    "\tseq_out = raw_text[i + seq_length] # The sequence position\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to transform it so that it is suitable for use with Keras. \n",
    "# First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "# Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "# Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. \n",
    "# This is so that we can configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each y value is converted into a sparse vector with a length of 47, \n",
    "# full of zeros except with a 1 in the column for the letter (integer) that the pattern represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 62, 63, 46, 48, 45, 40, 35, 33, 50, 2, 37, 51, 50, 35, 44, 32, 35, 48, 37, 64, 57, 59, 49, 2, 31, 42, 39, 33, 35, 64, 57, 59, 49, 2, 31, 34, 52, 35, 44, 50, 51, 48, 35, 49, 2, 39, 44, 2, 53, 45, 44, 34, 35, 48, 42, 31, 44, 34, 10, 2, 32, 55, 2, 42, 35, 53, 39, 49, 2, 33, 31, 48, 48, 45, 42, 42, 1, 0, 1, 0, 50, 38, 39, 49, 2, 35, 32, 45, 45, 41, 2, 39, 49, 2, 36, 45, 48, 2, 50]\n",
      "(173495, 100)\n"
     ]
    }
   ],
   "source": [
    "print(dataX[0])\n",
    "print(numpy.shape(dataX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173495, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "#print(X[0])\n",
    "print(numpy.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 3.0300Epoch 00000: loss improved from inf to 3.02994, saving model to weights-improvement-00-3.0299.hdf5\n",
      "173495/173495 [==============================] - 238s - loss: 3.0299   \n",
      "Epoch 2/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.7112Epoch 00001: loss improved from 3.02994 to 2.71111, saving model to weights-improvement-01-2.7111.hdf5\n",
      "173495/173495 [==============================] - 232s - loss: 2.7111   \n",
      "Epoch 3/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.5772Epoch 00002: loss improved from 2.71111 to 2.57716, saving model to weights-improvement-02-2.5772.hdf5\n",
      "173495/173495 [==============================] - 234s - loss: 2.5772   \n",
      "Epoch 4/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.5042Epoch 00003: loss improved from 2.57716 to 2.50424, saving model to weights-improvement-03-2.5042.hdf5\n",
      "173495/173495 [==============================] - 230s - loss: 2.5042   \n",
      "Epoch 5/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.4292Epoch 00004: loss improved from 2.50424 to 2.42927, saving model to weights-improvement-04-2.4293.hdf5\n",
      "173495/173495 [==============================] - 226s - loss: 2.4293   \n",
      "Epoch 6/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.3651Epoch 00005: loss improved from 2.42927 to 2.36498, saving model to weights-improvement-05-2.3650.hdf5\n",
      "173495/173495 [==============================] - 230s - loss: 2.3650   \n",
      "Epoch 7/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.3128Epoch 00006: loss improved from 2.36498 to 2.31288, saving model to weights-improvement-06-2.3129.hdf5\n",
      "173495/173495 [==============================] - 228s - loss: 2.3129   \n",
      "Epoch 8/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.3514Epoch 00007: loss did not improve\n",
      "173495/173495 [==============================] - 227s - loss: 2.3514   \n",
      "Epoch 9/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.2356Epoch 00008: loss improved from 2.31288 to 2.23560, saving model to weights-improvement-08-2.2356.hdf5\n",
      "173495/173495 [==============================] - 226s - loss: 2.2356   \n",
      "Epoch 10/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.2066Epoch 00009: loss improved from 2.23560 to 2.20658, saving model to weights-improvement-09-2.2066.hdf5\n",
      "173495/173495 [==============================] - 227s - loss: 2.2066   \n",
      "Epoch 11/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.1702Epoch 00010: loss improved from 2.20658 to 2.17029, saving model to weights-improvement-10-2.1703.hdf5\n",
      "173495/173495 [==============================] - 228s - loss: 2.1703   \n",
      "Epoch 12/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.1389Epoch 00011: loss improved from 2.17029 to 2.13895, saving model to weights-improvement-11-2.1390.hdf5\n",
      "173495/173495 [==============================] - 226s - loss: 2.1390   \n",
      "Epoch 13/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.1093Epoch 00012: loss improved from 2.13895 to 2.10928, saving model to weights-improvement-12-2.1093.hdf5\n",
      "173495/173495 [==============================] - 225s - loss: 2.1093   \n",
      "Epoch 14/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.0780Epoch 00013: loss improved from 2.10928 to 2.07803, saving model to weights-improvement-13-2.0780.hdf5\n",
      "173495/173495 [==============================] - 225s - loss: 2.0780   \n",
      "Epoch 15/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.0525Epoch 00014: loss improved from 2.07803 to 2.05251, saving model to weights-improvement-14-2.0525.hdf5\n",
      "173495/173495 [==============================] - 228s - loss: 2.0525   \n",
      "Epoch 16/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.0295Epoch 00015: loss improved from 2.05251 to 2.02940, saving model to weights-improvement-15-2.0294.hdf5\n",
      "173495/173495 [==============================] - 227s - loss: 2.0294   \n",
      "Epoch 17/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.0037Epoch 00016: loss improved from 2.02940 to 2.00375, saving model to weights-improvement-16-2.0037.hdf5\n",
      "173495/173495 [==============================] - 225s - loss: 2.0037   \n",
      "Epoch 18/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.9801Epoch 00017: loss improved from 2.00375 to 1.98011, saving model to weights-improvement-17-1.9801.hdf5\n",
      "173495/173495 [==============================] - 222s - loss: 1.9801   \n",
      "Epoch 19/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.9590Epoch 00018: loss improved from 1.98011 to 1.95904, saving model to weights-improvement-18-1.9590.hdf5\n",
      "173495/173495 [==============================] - 225s - loss: 1.9590   \n",
      "Epoch 20/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 1.9394Epoch 00019: loss improved from 1.95904 to 1.93944, saving model to weights-improvement-19-1.9394.hdf5\n",
      "173495/173495 [==============================] - 225s - loss: 1.9394   \n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating text with LSTM model\n",
    "### Firstly, we load the data and define the network in exactly the same way, except the network weights are loaded from a \n",
    "### checkpoint file and the network does not need to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-19-1.9394.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to \n",
    "# convert the integers back to characters so that we can understand the predictions.\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, \n",
    "# generate the next character then update the seed sequence to add the generated character on the end and trim off the \n",
    "# first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "('\"', 'd she dropped\\r\\nit hastily, just in time to avoid shrinking away altogether.\\r\\n\\r\\n\\xe2\\x80\\x98that was a narrow ', '\"')\n",
      "��� said the cotpouse, ���and the mort of the sooe\n",
      "to the mortee ��� \n",
      "\n",
      "���i dan���t tee toe toiereen ��� said the monk  ano oo   \n",
      "\n",
      "���what i den do a lirsle to too would toe toien of the sooe,��� she said \n",
      "to herself, ���the mort oo mere the morte oo the toie to the thitg oo \n",
      "the moote  soe moot oo the the the wort  th the to tee thi goot the was\n",
      "oote the wonle toied to the that sae hnre\n",
      "ant ho the rooee and the sooee of the sooe  \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# We can pick a random input pattern as our seed sequence, then print generated characters as we generate them.\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
